{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-13T00:38:22.671098100Z",
     "start_time": "2023-11-13T00:38:22.632100500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--max_prediction_length MAX_PREDICTION_LENGTH]\n",
      "                             [--max_encoder_length MAX_ENCODER_LENGTH]\n",
      "                             [--trainsize TRAINSIZE] [--validsize VALIDSIZE]\n",
      "                             [--out_threshold OUT_THRESHOLD] [--path PATH]\n",
      "                             [--tank_sample_id TANK_SAMPLE_ID]\n",
      "                             [--quantile QUANTILE] [--con CON]\n",
      "                             [--threshold_scale THRESHOLD_SCALE] [--step STEP]\n",
      "                             [--model_path MODEL_PATH] [--outfile OUTFILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-d2f4cae8-425e-48c0-87a3-4158f4c5ae89.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\u001B[1;31m:\u001B[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer, EncoderNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "from utils.AdaptiveThreshold import thresholding_algo\n",
    "sys.path.append('./')\n",
    "from ssa.btgym_ssa import SSA\n",
    "from evaluation import Evaluation_metrics\n",
    "parser = argparse.ArgumentParser(description='TFT on leakage datra')\n",
    "parser.add_argument('--max_prediction_length', type=int, default=2 * 24, help='forecast horizon')\n",
    "parser.add_argument('--max_encoder_length', type=int, default=3 * 2 * 24, help='past reference data')\n",
    "parser.add_argument('--trainsize', type=int, default=4000, help='train size')\n",
    "parser.add_argument('--validsize', type=int, default=500, help='validtaion size')\n",
    "parser.add_argument('--out_threshold', type=float, default=2, help='threshold for outlier filtering')\n",
    "parser.add_argument('--path', type=str, default='no_norm', help='TensorBoardLogger')\n",
    "parser.add_argument('--tank_sample_id', type=str, default='A205_1', help='tank sample for experiment')\n",
    "parser.add_argument('--quantile', type=float, default=0.985, help='threshold quantile')\n",
    "parser.add_argument('--con', type=int, default=15, help='consecutive counter')\n",
    "parser.add_argument('--threshold_scale', type=float, default=2, help='threshold scale')\n",
    "parser.add_argument('--step', type=int, default=12, help='step')\n",
    "parser.add_argument('--model_path', type=str,\n",
    "                    default='/no_normaliser/trial_16/epoch=48.ckpt', help='model_path')\n",
    "parser.add_argument('--outfile', type=str, default='no_norm', help='step')\n",
    "args = parser.parse_args()\n",
    "\n",
    "max_prediction_length = args.max_prediction_length\n",
    "max_encoder_length = args.max_encoder_length\n",
    "test_sequence = pd.read_csv('pytorch_forecasting/CPD/tl.csv')\n",
    "test_sequence = test_sequence[test_sequence['period'] == 0]\n",
    "test_sequence['period'] = test_sequence['period'].astype(str)\n",
    "TRAINSIZE = args.trainsize\n",
    "VALIDSIZE = args.validsize\n",
    "data = test_sequence[lambda x: x.time_idx <= TRAINSIZE + VALIDSIZE]\n",
    "data = data[abs(data['Var_tc_readjusted']) < args.out_threshold]\n",
    "tlgrouths = pd.read_csv('pytorch_forecasting/CPD/bottom02_info.csv',\n",
    "                        index_col=0).reset_index(drop=True)\n",
    "\n",
    "processed_dfs = []\n",
    "groups = data.groupby('group_id')\n",
    "window_size = 10\n",
    "for group_id, group_df in groups:\n",
    "    group_df = group_df.reset_index(drop=True)\n",
    "    group_df['time_idx'] = group_df.index\n",
    "    processed_dfs.append(group_df)\n",
    "final_df = pd.concat(processed_dfs, ignore_index=True)\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    final_df[lambda x: x.time_idx <= 3750],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"Var_tc_readjusted\",  # variance\n",
    "    group_ids=[\"group_id\"],  # tank id\n",
    "    min_encoder_length=max_encoder_length,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group_id\"],  # tank id, tank location state\n",
    "    static_reals=[\"tank_max_height\", \"tank_max_volume\"],\n",
    "    # tank max height, tank max volume, no. of pumps attached to the tank\n",
    "    time_varying_known_categoricals=[\"Time_of_day\"],\n",
    "    # season, month, remove \"Month\", \"Year\", \"Season\" if use only a month of data for training\n",
    "    time_varying_known_reals=[\"time_idx\"],  # time_idx,\n",
    "    time_varying_unknown_categoricals=[],  # period (idle, transaction, delivery)\n",
    "    time_varying_unknown_reals=[\n",
    "        \"Var_tc_readjusted\",\n",
    "        \"ClosingHeight_tc_readjusted\",\n",
    "        \"ClosingStock_tc_readjusted\",\n",
    "        \"TankTemp\",\n",
    "    ],  # variance, volume, height, sales(-), delivery(+), temperature, \"Del_tc\", \"Sales_Ini_tc\",\n",
    "    # target_normalizer=GroupNormalizer(\n",
    "    #     groups=[\"group_id\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus and normalize by group\n",
    "    target_normalizer=EncoderNormalizer(\n",
    "        method='robust',\n",
    "        max_length=None,\n",
    "        center=True,\n",
    "        transformation=None,\n",
    "        method_kwargs={}\n",
    "    ),\n",
    "    # target_normalizer=EncoderNormalizer(\n",
    "    #     method='robust',\n",
    "    #     center=False\n",
    "    # ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True\n",
    ")\n",
    "validation = TimeSeriesDataSet.from_dataset(training, final_df, predict=True, stop_randomization=True)\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=args.path)  # logging results to a tensorboard\n",
    "quantile_levels = [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "def loss(y_pred, target):\n",
    "    # calculate quantile loss\n",
    "    losses = []\n",
    "    for i, q in enumerate(quantile_levels):\n",
    "        errors = target - y_pred[..., i]\n",
    "        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(-1))\n",
    "    losses = 2 * torch.cat(losses, dim=2)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    path = os.getcwd() + args.model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(path)\n",
    "    training_cutoff = 2000 - max_prediction_length\n",
    "    no_CPs = 0\n",
    "    no_preds = 0\n",
    "    no_TPS = 0\n",
    "    delays = []\n",
    "    runtime = []\n",
    "    error_margin = 864000\n",
    "\n",
    "    if not os.path.exists(args.outfile):\n",
    "        os.makedirs(args.outfile)\n",
    "    dones = [f for f in os.listdir(args.outfile + '/') if os.path.isfile(os.path.join(args.outfile + '/', f))]\n",
    "    dones = [f[:6] for f in dones]\n",
    "\n",
    "    for tank_sample_id in list(test_sequence['group_id'].unique()):\n",
    "        if tank_sample_id in ['A043_2','A239_2','A441_2', 'B402_3', 'B402_4', 'F249_1', 'F257_2', 'F289_4', 'F406_1', 'J813_2']:\n",
    "            continue\n",
    "        if tank_sample_id in dones:\n",
    "            continue\n",
    "        if os.path.isfile(args.outfile + '.npz'):\n",
    "            data = np.load(args.outfile + '.npz')\n",
    "            no_CPs, no_preds, no_TPS = data['no_CPs'], data['no_preds'], data['no_TPS']\n",
    "        tank_sequence = test_sequence[(test_sequence['group_id'] == tank_sample_id)]\n",
    "        tank_sequence = tank_sequence[tank_sequence['period'] == '0']\n",
    "        train_seq = tank_sequence.iloc[:training_cutoff]\n",
    "        train_seq = train_seq[abs(train_seq['Var_tc_readjusted']) < args.out_threshold]\n",
    "        train_seq = train_seq.reset_index(drop=True)\n",
    "        train_seq['time_idx'] = train_seq.index\n",
    "        X = np.array(train_seq['Var_tc_readjusted'].values)\n",
    "        ssa = SSA(window=5, max_length=len(X))\n",
    "        X_pred = ssa.reset(X)\n",
    "        X_pred = ssa.transform(X_pred, state=ssa.get_state())\n",
    "        reconstructeds = X_pred.sum(axis=0)\n",
    "        residuals = X - reconstructeds\n",
    "        resmean = residuals.mean()\n",
    "        M2 = ((residuals - resmean) ** 2).sum()\n",
    "\n",
    "        tn = TimeSeriesDataSet.from_dataset(training, train_seq, stop_randomization=True)\n",
    "        train_dataloader = tn.to_dataloader(train=False, batch_size=128, num_workers=0)\n",
    "        train_predictions = best_tft.predict(train_dataloader, mode=\"raw\", return_x=True,\n",
    "                                             trainer_kwargs=dict(accelerator=\"gpu\"))\n",
    "        trainpred = train_predictions.output[\"prediction\"][:, :, :]\n",
    "        traintarget = train_predictions.x[\"decoder_target\"][:, :]\n",
    "        quantile_loss = loss(trainpred, traintarget)\n",
    "        quantile_loss = torch.sum(quantile_loss, dim=2)\n",
    "        quantile_loss, _ = torch.median(quantile_loss, dim=1)\n",
    "        # MSE = torch.mean((trainpred - traintarget) ** 2, dim=1)\n",
    "        base = torch.quantile(quantile_loss, args.quantile)\n",
    "        final_threshold = args.threshold_scale * base\n",
    "        test_seq = tank_sequence.iloc[training_cutoff:]\n",
    "        test_seq = test_seq.reset_index(drop=True)\n",
    "        test_seq['time_idx'] = test_seq.index\n",
    "        step = args.step\n",
    "        ts = pd.to_datetime(test_seq['Time'])\n",
    "        scores = [0] * test_seq.shape[0]\n",
    "        errors = quantile_loss\n",
    "        thresholds = [final_threshold] * test_seq.shape[0]\n",
    "        outliers = []\n",
    "        filtered = []\n",
    "        site_id = tank_sample_id[:4]\n",
    "        tank_id = tank_sample_id[-1]\n",
    "        tank_info = tlgrouths[(tlgrouths['Site'] == site_id) & (tlgrouths['Tank'] == int(tank_id))]\n",
    "        startdate = tank_info.iloc[0]['Start_date']\n",
    "        stopdate = tank_info.iloc[0]['Stop_date']\n",
    "        temp_df = test_seq[test_seq['Time_DN'] > startdate]\n",
    "        startindex = temp_df.iloc[0]['time_idx']\n",
    "        temp_df = test_seq[test_seq['Time_DN'] > stopdate]\n",
    "        stopindex = temp_df.iloc[0]['time_idx']\n",
    "        gt_margin = []\n",
    "        gt_margin.append((ts[startindex-10], ts[startindex] + pd.to_timedelta(7, unit='D'), ts[startindex]))\n",
    "        gt_margin.append((ts[stopindex-10], ts[stopindex] + pd.to_timedelta(7, unit='D'), ts[stopindex]))\n",
    "        ctr = 0\n",
    "        while ctr < test_seq.shape[0]:\n",
    "            new = test_seq['Var_tc_readjusted'].iloc[ctr:ctr + step].values\n",
    "            updates = ssa.update(new)\n",
    "            updates = ssa.transform(updates, state=ssa.get_state())[:, 5 - 1:]\n",
    "            reconstructed = updates.sum(axis=0)\n",
    "            residual = new - reconstructed\n",
    "            residuals = np.concatenate([residuals, residual])\n",
    "            # start_time = time.time()\n",
    "            for i1 in range(len(new)):\n",
    "                if new[i1] > 1 or new[i1] < -1:\n",
    "                    outliers.append(ctr + i1)\n",
    "                    filtered.append(np.mean(filtered[-5:] if len(filtered) > 5 else 0))\n",
    "                else:\n",
    "                    delta = residual[i1] - resmean\n",
    "                    resmean += delta / (ctr + i1 + training_cutoff)\n",
    "                    M2 += delta * (residual[i1] - resmean)\n",
    "                    stdev = math.sqrt(M2 / (ctr + i1 + training_cutoff - 1))\n",
    "                    threshold_upper = resmean + 2 * stdev\n",
    "                    threshold_lower = resmean - 2 * stdev\n",
    "\n",
    "                    if (residual[i1] <= threshold_upper) and (residual[i1] >= threshold_lower):\n",
    "                        filtered.append(new[i1])\n",
    "                    else:\n",
    "                        outliers.append(ctr + i1)\n",
    "                        filtered.append(np.mean(filtered[-5:] if len(filtered) > 5 else 0))\n",
    "            test_seq.loc[ctr:ctr + step - 1, 'Var_tc_readjusted'] = filtered[-step:]\n",
    "            ctr += step\n",
    "            if ctr + step >= test_seq.shape[0]:\n",
    "                break\n",
    "\n",
    "        test = TimeSeriesDataSet.from_dataset(training, test_seq, stop_randomization=True)\n",
    "        test_dataloader = test.to_dataloader(train=False, batch_size=128, num_workers=0)\n",
    "        new_raw_predictions = best_tft.predict(test_dataloader, mode=\"raw\", return_x=True,\n",
    "                                       trainer_kwargs=dict(accelerator=\"gpu\"))\n",
    "        onepred = new_raw_predictions.output[\"prediction\"][:, :, :]\n",
    "        onetarget = new_raw_predictions.x[\"decoder_target\"][:, :]\n",
    "        quantile_loss = loss(onepred, onetarget)\n",
    "        quantile_loss = torch.sum(quantile_loss, dim=2)\n",
    "        quantile_loss, _ = torch.median(quantile_loss, dim=1)\n",
    "        # mse_values = torch.mean((onepred - onetarget) ** 2, dim=1)\n",
    "        ctr = max_encoder_length\n",
    "        while ctr < len(quantile_loss)-max_prediction_length:\n",
    "            mse_ind = ctr-max_encoder_length\n",
    "            mv = quantile_loss[mse_ind:mse_ind+step]\n",
    "            errors = torch.cat((errors, mv), dim=0)\n",
    "            mse_quantile = torch.quantile(errors[:-args.step], args.quantile)\n",
    "            final_threshold = args.threshold_scale * mse_quantile\n",
    "            thresholds[ctr:ctr + step] = [final_threshold] * step\n",
    "            scores[ctr:ctr + step] = mv\n",
    "            ctr += step\n",
    "            if ctr + step >= test_seq.shape[0]:\n",
    "                ss = test_seq.shape[0] - str\n",
    "                mse_ind = ctr - max_encoder_length\n",
    "                mv = quantile_loss[mse_ind:mse_ind + ss]\n",
    "                errors = torch.cat((errors, mv), dim=0)\n",
    "                mse_quantile = torch.quantile(errors, args.quantile)\n",
    "                final_threshold = args.threshold_scale * mse_quantile\n",
    "                thresholds[ctr:ctr + ss] = [final_threshold] * ss\n",
    "                scores[ctr:ctr + ss] = mv\n",
    "\n",
    "            # if ctr >= max_prediction_length + max_encoder_length:\n",
    "            #     new_prediction_data = test_seq[ctr + step - max_prediction_length - max_encoder_length:ctr + step]\n",
    "            #     new_raw_predictions = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "            #     onepred = new_raw_predictions.output[\"prediction\"][:, :, 3]\n",
    "            #     onetarget = new_raw_predictions.x[\"decoder_target\"][:, :]\n",
    "            #     mse_values = torch.mean((onepred - onetarget) ** 2, dim=1)\n",
    "            #     errors = np.append(errors, mse_values)\n",
    "            #     mse_quantile = np.quantile(errors, args.quantile)\n",
    "            #     final_threshold = args.threshold_scale * mse_quantile\n",
    "            #     thresholds[ctr:ctr + step] = [final_threshold] * step\n",
    "            #     scores[ctr:ctr + step] = [mse_values] * step\n",
    "\n",
    "\n",
    "\n",
    "        # determine the results of prediction\n",
    "        # scores = [0] * max_encoder_length + scores + [0] * max_prediction_length\n",
    "        # thresholds = [0] * max_encoder_length + thresholds + [0] * max_prediction_length\n",
    "        preds = [idx+args.max_prediction_length//2 for idx in range(len(scores)) if scores[idx] > thresholds[idx]]\n",
    "        preds = find_middle_values(preds)\n",
    "        scores = [tt.item() if tt != 0 else 0 for tt in scores]\n",
    "        thresholds = [tt.item() if tt != 0 else 0 for tt in thresholds]\n",
    "\n",
    "        no_CPs += 2\n",
    "        no_preds += len(preds)\n",
    "        mark = []\n",
    "        for j in preds:\n",
    "            timestamp = ts[j]\n",
    "            for l in gt_margin:\n",
    "                if timestamp >= l[0] and timestamp <= l[1]:\n",
    "                    if l not in mark:\n",
    "                        mark.append(l)\n",
    "                    else:\n",
    "                        no_preds -= 1\n",
    "                        continue\n",
    "                    no_TPS += 1\n",
    "                    delays.append(timestamp - l[2])\n",
    "        np.savez(args.outfile, no_CPs=no_CPs, no_preds=no_preds, no_TPS=no_TPS)\n",
    "        filtered = filtered + [0] * (len(ts) - len(filtered))\n",
    "        fig = plt.figure()\n",
    "        fig, ax = plt.subplots(2, figsize=[18, 16], sharex=True)\n",
    "        ax[0].plot(ts.array, filtered)\n",
    "        ax[0].axvline(x=ts[startindex], color='green', linestyle='--')\n",
    "        ax[0].axvline(x=ts[stopindex], color='green', linestyle='--')\n",
    "        for cp in preds:\n",
    "            ax[0].axvline(x=ts[cp], color='purple', alpha=0.6)\n",
    "        ax[1].scatter(ts.array, scores)\n",
    "        ax[1].scatter(ts.array, thresholds)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(args.outfile + '/' + tank_sample_id + '.png')\n",
    "        plt.close('all')\n",
    "        del fig\n",
    "\n",
    "    rec = Evaluation_metrics.recall(no_TPS, no_CPs)\n",
    "    FAR = Evaluation_metrics.False_Alarm_Rate(no_preds, no_TPS)\n",
    "    prec = Evaluation_metrics.precision(no_TPS, no_preds)\n",
    "    f1score = Evaluation_metrics.F1_score(rec, prec)\n",
    "    f2score = Evaluation_metrics.F2_score(rec, prec)\n",
    "    # dd = Evaluation_metrics.detection_delay(delays)\n",
    "    print('recall: ', rec)\n",
    "    print('false alarm rate: ', FAR)\n",
    "    print('precision: ', prec)\n",
    "    print('F1 Score: ', f1score)\n",
    "    print('F2 Score: ', f2score)\n",
    "    # print('detection delay: ', dd)\n",
    "\n",
    "    npz_filename = args.outfile\n",
    "    np.savez(npz_filename, rec=rec, FAR=FAR, prec=prec, f1score=f1score, f2score=f2score)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c81bde7bf3806342"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
